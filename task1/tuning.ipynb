{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuVAh9mueBTC"
      },
      "source": [
        "# Selecting hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "iUtPIeBoZxEB"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.config.run_functions_eagerly(True) #this solves some issues with running collab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndpCDvm6gPWI"
      },
      "source": [
        "## Import all needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QapYcKGbPAgg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from abc import ABC, abstractmethod"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lebAFlN7gVhP"
      },
      "source": [
        "## Implementing interface and  Grid Search for Random Forest Classifier\n",
        "\n",
        "This code defines an abstract classifier interface for MNIST models.  \n",
        "RandomForestMnistClassifier extends it and uses **GridSearchCV** ([docs](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)) to find the best hyperparameters for RandomForestClassifier. After tuning, it trains the best model on the full dataset and uses it for predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "a35-bcpiek27"
      },
      "outputs": [],
      "source": [
        "class MnistClassifierInterface(ABC):\n",
        "    @abstractmethod\n",
        "    def train(self, X_train, y_train):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict(self, X_test):\n",
        "        pass\n",
        "\n",
        "class RandomForestMnistClassifier(MnistClassifierInterface):\n",
        "    def __init__(self):\n",
        "        self.model = RandomForestClassifier(n_jobs=-1)\n",
        "        self.best = None\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        params = {'n_estimators': [100,200],'max_depth': [10,20, None],'min_samples_split': [2, 4],'min_samples_leaf': [1,2],'bootstrap': [True, False]}\n",
        "        gs = GridSearchCV(self.model, params, scoring='accuracy', n_jobs=-1, cv=3, verbose=2)\n",
        "        gs.fit(X_train, y_train)\n",
        "        self.best = gs.best_estimator_\n",
        "        self.best.fit(X_train, y_train)\n",
        "        print(self.best)\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        return self.best.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kl9uOGbUioZk"
      },
      "source": [
        "### Data preparation\n",
        "\n",
        "Here I prepare data for training. This format is suitable for **all** models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEu8Vx0ifCc-",
        "outputId": "d48f4d14-ea5b-4361-ff78-1e8c83e3e46b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(X_train.shape[0], -1) / 255.0\n",
        "X_test = X_test.reshape(X_test.shape[0], -1) / 255.0\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8r5XynzvloUp"
      },
      "source": [
        "### The best params for Random forest and accuracy evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMilNNjlfIYj",
        "outputId": "caf1b52f-8c60-4e13-e115-6dfb2fad7527"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
            "RandomForestClassifier(bootstrap=False, n_estimators=200, n_jobs=-1)\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.98       980\n",
            "           1       0.99      0.99      0.99      1135\n",
            "           2       0.97      0.97      0.97      1032\n",
            "           3       0.97      0.97      0.97      1010\n",
            "           4       0.98      0.98      0.98       982\n",
            "           5       0.98      0.97      0.98       892\n",
            "           6       0.98      0.98      0.98       958\n",
            "           7       0.97      0.97      0.97      1028\n",
            "           8       0.96      0.96      0.96       974\n",
            "           9       0.96      0.95      0.96      1009\n",
            "\n",
            "    accuracy                           0.97     10000\n",
            "   macro avg       0.97      0.97      0.97     10000\n",
            "weighted avg       0.97      0.97      0.97     10000\n",
            "\n",
            "Random Forest Test Accuracy: 0.9739\n"
          ]
        }
      ],
      "source": [
        "rf = RandomForestMnistClassifier()\n",
        "rf.train(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
        "print(f\"Random Forest Test Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpR06GH7l7x3"
      },
      "source": [
        "## Implementing and finding parameters for Feed Forward neural network\n",
        "\n",
        "FeedForwardMnistClassifier extends interface abive and uses [Keras Tuner](https://www.tensorflow.org/tutorials/keras/keras_tuner) (there is even an example with similar to MNIST dataset) with **Hyperband** to find the best hyperparameters for a feedforward neural network model. After tuning, it trains the best model on the dataset and uses it for predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vJ7JnVkgoZG",
        "outputId": "8119c5a6-34db-4d16-9118-1eee541b8c8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting keras_tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from keras_tuner) (3.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras_tuner) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from keras_tuner) (2.32.3)\n",
            "Collecting kt-legacy (from keras_tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (0.14.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->keras_tuner) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->keras_tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->keras_tuner) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->keras_tuner) (2025.1.31)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras->keras_tuner) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras_tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras_tuner) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras_tuner) (0.1.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m122.9/129.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras_tuner\n",
            "Successfully installed keras_tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip install keras_tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0dW8jCHvnqtA"
      },
      "outputs": [],
      "source": [
        "import keras_tuner as kt\n",
        "\n",
        "class FeedForwardMnistClassifier(MnistClassifierInterface):\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "\n",
        "    def build_model(self, hp):\n",
        "        model = models.Sequential()\n",
        "        model.add(layers.Dense(units=hp.Int('units1', min_value=128, max_value=256, step=32), activation='relu', input_shape=(28*28,)))\n",
        "        model.add(layers.Dropout(rate=hp.Float('dropout1', min_value=0.1, max_value=0.3, step=0.1)))\n",
        "        model.add(layers.Dense(units=hp.Int('units2', min_value=64, max_value=128, step=32), activation='relu'))\n",
        "        model.add(layers.Dropout(rate=hp.Float('dropout2', min_value=0.1, max_value=0.3, step=0.1)))\n",
        "        model.add(layers.Dense(10, activation='softmax'))\n",
        "        model.compile(optimizer=hp.Choice('optimizer', values=['adam', 'rmsprop']), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def train(self, X_train, y_train, epochs=10, batch_size=64):\n",
        "        y_train = to_categorical(y_train, 10)\n",
        "        tuner = kt.Hyperband(self.build_model, objective='val_accuracy', max_epochs=epochs, factor=3, directory='mybin1', project_name='mnist_tuning')\n",
        "        tuner.search(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
        "        best_hp = tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters\n",
        "        for hp_name in best_hp.values:\n",
        "            print(f\"{hp_name}: {best_hp.get(hp_name)}\")\n",
        "        self.model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        return np.argmax(self.model.predict(X_test), axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdzMOgNGo-ZN"
      },
      "source": [
        "### The best params for Feed-Forward Neural Network and accuracy evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "qx3z9BrHn4bQ",
        "outputId": "fed75411-30bd-4162-ea57-9ec891ed0e1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 30 Complete [00h 00m 39s]\n",
            "val_accuracy: 0.9779629707336426\n",
            "\n",
            "Best val_accuracy So Far: 0.9790740609169006\n",
            "Total elapsed time: 00h 08m 30s\n",
            "units1: 192\n",
            "dropout1: 0.1\n",
            "units2: 96\n",
            "dropout2: 0.1\n",
            "optimizer: adam\n",
            "tuner/epochs: 10\n",
            "tuner/initial_epoch: 4\n",
            "tuner/bracket: 1\n",
            "tuner/round: 1\n",
            "tuner/trial_id: 0019\n"
          ]
        }
      ],
      "source": [
        "classifier = FeedForwardMnistClassifier()\n",
        "classifier.train(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ke6xvc4gpRKj"
      },
      "source": [
        "## Implementing and finding parameters for Convolutional neural network\n",
        "\n",
        "Here I did all the same as above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XTcvavyg9g3B"
      },
      "outputs": [],
      "source": [
        "class CNNMnistClassifier(MnistClassifierInterface):\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "\n",
        "    def build_model(self, hp):\n",
        "        model = models.Sequential()\n",
        "        model.add(layers.Reshape((28, 28, 1), input_shape=(28*28,)))\n",
        "        model.add(layers.Conv2D(filters=hp.Int('filters1', min_value=32, max_value=128, step=32),kernel_size=hp.Choice('kernel_size1', values=[3, 5]),activation='relu',input_shape=(28, 28, 1)))\n",
        "        model.add(layers.MaxPooling2D(pool_size=2))\n",
        "        model.add(layers.Conv2D(filters=hp.Int('filters2', min_value=64, max_value=128, step=32),kernel_size=hp.Choice('kernel_size2', values=[3, 5]),activation='relu'))\n",
        "        model.add(layers.MaxPooling2D(pool_size=2))\n",
        "        model.add(layers.Flatten())\n",
        "        model.add(layers.Dense(units=hp.Int('dense_units', min_value=64, max_value=256, step=64),activation='relu'))\n",
        "        model.add(layers.Dropout(rate=hp.Float('dropout', min_value=0.2, max_value=0.4, step=0.1)))\n",
        "        model.add(layers.Dense(10, activation='softmax'))\n",
        "        model.compile( optimizer=hp.Choice('optimizer', values=['adam', 'rmsprop']), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def train(self, X_train, y_train, epochs=10, batch_size=64):\n",
        "        y_train = to_categorical(y_train, 10)\n",
        "        tuner = kt.Hyperband(self.build_model, objective='val_accuracy', max_epochs=epochs, factor=3)\n",
        "        tuner.search(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=1)\n",
        "        self.model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        return np.argmax(self.model.predict(X_test), axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9jinZJxpqvM"
      },
      "source": [
        "### The best params for Feed-Forward Neural Network and accuracy evaluation\n",
        "\n",
        "Unfortunately, I didn't finish because google collab runtime ends. BUT I GOT SOME RESULTS!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9vFlbcT_41c",
        "outputId": "85614213-0cba-4f53-9f81-f484ff6329f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 21 Complete [00h 16m 34s]\n",
            "val_accuracy: 0.9912037253379822\n",
            "\n",
            "Best val_accuracy So Far: 0.9918518662452698\n",
            "Total elapsed time: 01h 22m 25s\n",
            "\n",
            "Search: Running Trial #22\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "32                |96                |filters1\n",
            "3                 |3                 |kernel_size1\n",
            "96                |128               |filters2\n",
            "3                 |5                 |kernel_size2\n",
            "64                |192               |dense_units\n",
            "0.3               |0.3               |dropout\n",
            "rmsprop           |adam              |optimizer\n",
            "10                |4                 |tuner/epochs\n",
            "4                 |0                 |tuner/initial_epoch\n",
            "1                 |1                 |tuner/bracket\n",
            "1                 |0                 |tuner/round\n",
            "0014              |None              |tuner/trial_id\n",
            "\n",
            "Epoch 5/10\n",
            "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 60ms/step - accuracy: 0.9883 - loss: 0.0383 - val_accuracy: 0.9886 - val_loss: 0.0444\n",
            "Epoch 6/10\n",
            "\u001b[1m191/675\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 56ms/step - accuracy: 0.9916 - loss: 0.0270"
          ]
        }
      ],
      "source": [
        "classifier = CNNMnistClassifier()\n",
        "classifier.train(X_train, y_train)\n",
        "\n",
        "best_hps = classifier.model.get_config()\n",
        "print(\"\\nBest Hyperparameters:\")\n",
        "for layer in best_hps['layers']:\n",
        "    print(layer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
